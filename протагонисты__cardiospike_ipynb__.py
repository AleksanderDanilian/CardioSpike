# -*- coding: utf-8 -*-
"""Протагонисты - "CardioSpike.ipynb""

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rCjtrWUHlK3BT7LQPPKdywK1DQIs-yH7
"""

# Commented out IPython magic to ensure Python compatibility.
from tensorflow.keras.models import Model, Sequential # загружаем абстрактный класс базовой модели сети от кераса и последовательную модель
# Из кераса загружаем необходимые слои для нейросети
from tensorflow.keras.layers import Dense, Flatten, Reshape, Input, Conv2DTranspose, \
concatenate, Activation, MaxPooling2D, Conv2D, BatchNormalization, Conv1D, MaxPooling1D, Lambda
from tensorflow.keras import backend as K # подтягиваем базовые керасовские функции
from tensorflow.keras.optimizers import Adam # загружаем выбранный оптимизатор
from tensorflow.keras import utils # загружаем утилиты кераса
from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint
import tensorflow as tf

import matplotlib.pyplot as plt # из библиотеки для визуализации данных возьмём интерфейс для построения графиков простых функций
from tensorflow.keras.preprocessing import image # модуль для отрисовки изображения
import numpy as np # библиотека для работы с массивами данных
import pandas as pd # библиотека для анализа и обработки данных
from PIL import Image # модуль для отрисовки изображения
from sklearn.model_selection import train_test_split # модуль для разбивки выборки на тренировочную/тестовую
from sklearn.preprocessing import StandardScaler # модуль для стандартизации данных
from sklearn.metrics import f1_score

import warnings
import random
import os #
from google.colab import drive #модуль для работы с google диском 
import time
import random
import csv
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

dfBase = pd.read_csv('/content/train.csv')

dfBase

df = dfBase.copy()

df = df[:55084] # остальное на тест

cvdDf = df[df['y']==1]

cvdDf

# получим списки значений x для ковид участков и индексы DataFrame им соответсвующие

cvdLst = []
idxLst = []
cases = 275

for j in range(cases):
  cvdLst.append(cvdDf[cvdDf['id']==j]['x'].values)
  idxLst.append(cvdDf[cvdDf['id']==j].index.values)

# соберем все сигналы ковида по отдельным массивам
cvdLstSp = [] #CovidListSeparate
cvdIndSp = [] #Indexes separate

for i in range(cases):
  temp = []
  tCount = 0
  for j in range(len(idxLst[i])-1):
    if idxLst[i][j+1] > idxLst[i][j] + 1:
      cvdLstSp.append(cvdLst[i][tCount:j+1])
      cvdIndSp.append(idxLst[i][tCount:j+1])
      tCount = j+1
    if j == (len(idxLst[i])-2):
      cvdLstSp.append(cvdLst[i][tCount:j+2])
      cvdIndSp.append(idxLst[i][tCount:j+2])

# напишем ф-цию, которая будет увеличивать "ковид-интервалы" до требуемой длины окна.

def getCvdIntervals(window = 41, cvdIndSp = cvdIndSp, df = df):

  cvdLstOSidxs = [] #covid List One Size indexes

  for i in range(len(cvdIndSp)):
    
    length = len(cvdIndSp[i])
    if length < window:
      increase = window - length
      if increase % 2 == 0:
        temp = [i for i in range(min(cvdIndSp[i]) - int((window - length)/2), max(cvdIndSp[i]) + int((window - length)/2) + 1)]
      if increase % 2 == 1:
        temp = [i for i in range(min(cvdIndSp[i]) - int((window - length)/2), max(cvdIndSp[i]) + int((window - length)/2) + 2)]
    
    elif length > window:
      decrease = length - window
      if decrease % 2 == 0:
        temp = [i for i in range(min(cvdIndSp[i]) + int((length - window)/2), max(cvdIndSp[i]) - int((length - window)/2) + 1 )]
      if decrease % 2 == 1:
        temp = [i for i in range(min(cvdIndSp[i]) + int((length - window)/2), max(cvdIndSp[i]) - int((length - window)/2) )]

    else:
      temp = cvdIndSp[i]   # window == length

    
    cvdLstOSidxs.append(temp)
    
  cvdLstOSrr = [] # covid List One Size RR intervals

  
  for i in range(len(cvdLstOSidxs)):
    
      cvdLstOSrr.append(df.iloc[cvdLstOSidxs[i],2].values)

  return cvdLstOSrr, cvdLstOSidxs

def getNrmlIntervals(window = 6, df = df, idxs = idxLst, step=10, cases = 237):
  # idxLst - список ковидных аномалий (индексы из базы df)
  # step - шаг нарезки данных. Уменьшить шаг - больше данных.
  # window - ширина окна данных

  xVals = []
  idxs = []
  cases = cases

  for j in range(cases):
    xVals.append(df[df['id']==j]['x'].values)
    idxs.append(df[df['id']==j].index.values)

  nrmlIntervals = []
  for i in range(cases):
    temp = []
    
    for k in range(len(xVals[i])):
      if all(idxLst[i]!=idxs[i][k]) and len(xVals[i])!=0:   #если индекс не равен любому из списка ковидных
        temp.append(xVals[i][k])
        
      else:
        
        for z in range(0,len(temp) - window, step):
          z2 = z + window
          if len(temp[z:z2])==window:
            nrmlIntervals.append(temp[z:z2])
        temp = []

  return nrmlIntervals

# сформируем y

def getY(nrmLst = [], cvdLst = []):

  y = [0 for el in nrmLst]
  y1 = [1 for el in cvdLst]
  y.extend(y1)
  y = np.array(y)

  return y

# сформируем x

def getX(nrmLst = [], cvdLst = []):
  x = []
  x.extend(nrmLst)
  x.extend(cvdLst)

  return x

def augm(x, window):
  b = 1.03
  a = 0.97
  x = x*((b - a) * np.random.random_sample(window) + a)
  return x

# Ф-ция подготовки данных

def getXY(window = 8, df = df, idxs = idxLst, cvdIndSp = cvdIndSp, step=85, test_size = 0.2, Conv = False):

  cvdLst,_ = getCvdIntervals(window = window, cvdIndSp=cvdIndSp, df = df)
  nrmLst = getNrmlIntervals(window = window, df = df, idxs = idxs, step = step)

  for i in range(len(cvdLst)):
    for j in range(15):
      temp = augm(cvdLst[i], window=window)
      cvdLst.append(temp)
  
  for i in range(len(nrmLst)):
    for j in range(10):
      temp = augm(nrmLst[i], window = window)
      nrmLst.append(temp)


  Y = getY(nrmLst = nrmLst, cvdLst = cvdLst)
  X = getX(nrmLst = nrmLst, cvdLst = cvdLst)

  X = np.array(X)
  Y = np.array(Y)

  scaler = StandardScaler()
  X = scaler.fit_transform(X)  

  if Conv:
    X = X.reshape((X.shape[0],X.shape[1],1))

  print(X.shape, Y.shape)

  x_train, x_val, y_train, y_val = train_test_split(X, Y, test_size = test_size, shuffle = True)  

  return x_train, x_val, y_train, y_val, scaler

#ДАННЫЕ ДЛЯ 1 МОДЕЛИ (scaler8 - окно 8, шаг влияет на вел-ну выборки)

window = 8 
step = 3

x_train1, x_val1, y_train1, y_val1, scaler8 = getXY(Conv = True, window = window, test_size=0.02, step = step)

model1 = Sequential()

model1.add(Conv1D(256, 5, padding = 'same', activation = 'linear'))
model1.add(Conv1D(128, 5, padding = 'same', activation = 'linear'))

#model1.add(MaxPooling1D())
model1.add(Conv1D(128, 5, padding = 'same', activation = 'linear'))
model1.add(Conv1D(64, 5, padding = 'same', activation = 'linear'))

#model1.add(MaxPooling1D())
#model1.add(Conv1D(128, 5, padding = 'same', activation = 'linear'))
#model1.add(Conv1D(128, 5, padding = 'same', activation = 'linear'))

#model1.add(MaxPooling1D())
#model1.add(Conv1D(128, 5, padding = 'same', activation = 'sigmoid'))
#model1.add(Conv1D(128, 5, padding = 'same', activation = 'sigmoid'))

model1.add(Flatten())

model1.add(Dense(2000, activation = 'sigmoid'))
model1.add(Dense(750, activation = 'sigmoid'))

model1.add(Dense(1, activation = 'sigmoid'))

model1.compile(loss = 'binary_crossentropy', optimizer = Adam(0.001), metrics = ['accuracy'])

reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,
                              patience=5, min_lr=0.0000001)

model1.fit(x_train1, y_train1, batch_size = 128, epochs = 100, validation_data = (x_val1, y_val1), callbacks=[reduce_lr])

#ДАННЫЕ ДЛЯ 2 МОДЕЛИ (scaler10 - окно 10, шаг влияет на вел-ну выборки)

window = 10 
step = 3

x_train2, x_val2, y_train2, y_val2, scaler10 = getXY(Conv = True, window = window, test_size=0.02, step = step)

model2 = Sequential()

model2.add(Conv1D(256, 5, padding = 'same', activation = 'elu'))
model2.add(Conv1D(128, 5, padding = 'same', activation = 'elu'))

#model2.add(MaxPooling1D())
model2.add(Conv1D(128, 5, padding = 'same', activation = 'elu'))
model2.add(Conv1D(32, 5, padding = 'same', activation = 'elu'))

model2.add(Flatten())

model2.add(Dense(2000, activation = 'sigmoid'))
model2.add(Dense(750, activation = 'sigmoid'))

model2.add(Dense(1, activation = 'sigmoid'))

model2.compile(loss = 'binary_crossentropy', optimizer = Adam(0.001), metrics = ['accuracy'])

reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,
                              patience=5, min_lr=0.0000001)

model2.fit(x_train2, y_train2, batch_size = 128, epochs = 100, validation_data = (x_val2, y_val2), callbacks=[reduce_lr])

res = model1.predict(x_val1)

predCorrect = 0
for i in range(len(res)):
  measurements = len(x_val1)
  if int(round(res[i][0],0)) == y_val1[i]:
    predCorrect +=1

finalAccuracy = predCorrect * 100 / measurements

print(finalAccuracy)

# ф-ция для заполнения файла

pathIn = '/content/test.csv' ## будет другое имя файла
pathOut = '/content/submission.csv'

dfTst = pd.read_csv(pathIn)

# фция подготовки файла к отправке на сервер

def check(df = dfTst, threshold = 3.0, window1 = 10, window2 = 8, scaler1 = scaler10, scaler2 = scaler8):

  totalScore = []
  totalTrueVals = []
  totalResultCum = []
  idLst = df['id'].unique()
  for i in idLst:
    tempX = []
    tempIdxs = []
    testLstX1 = []
    testLstIdx1 = []
    testLstX2 = []
    testLstIdx2 = []
    
    tempX.extend(df[df['id']==i]['x'].values) # формируем данные для подачи в нс
    tempIdxs.extend(df[df['id']==i].index.values)

    for j in range(0,len(tempX)-window1+1, 1):
      
      t = scaler1.transform([tempX[j:j + window1]]) 
      testLstX1.append(t)
      testLstIdx1.append(tempIdxs[j:j + window1])

    resultCum = [0.0 for el in range(len(tempX))]
    for k in range(len(testLstX1)): # для каждого из окон (8 значений со смещением 1)
      if len(testLstX1[k])!=0:
        testLstX1[k] = testLstX1[k].reshape((testLstX1[k].shape[0],testLstX1[k].shape[1],1))
        result1 = model1.predict(testLstX1[k])[0][0] # получаем предикты по окнам из 8 значений для одного кейса за раз.
        
        resultWindowSpread1 = np.concatenate([[0.0 for i in range(k)],[result1 for i in range(window1)],[0.0 for i in range(k+window1,len(tempX))]])
        
        resultCum = resultCum + resultWindowSpread1 

    for j in range(0,len(tempX)-window2+1, 1):
      
      t = scaler2.transform([tempX[j:j + window2]]) 
      testLstX2.append(t)
      testLstIdx2.append(tempIdxs[j:j+window2])

    for k in range(len(testLstX2)): # для каждого из окон (8 значений со смещением 1)
      if len(testLstX2[k])!=0:
        testLstX2[k] = testLstX2[k].reshape((testLstX2[k].shape[0],testLstX2[k].shape[1],1))
        
        result2 = model2.predict(testLstX2[k])[0][0] # 2я модель с другим окном
       
        resultWindowSpread2 = np.concatenate([[0.0 for i in range(k)],[result2 for i in range(window2)],[0.0 for i in range(k+window2,len(tempX))]])
        resultCum = resultCum + resultWindowSpread2


    if len(resultCum)!=0: ## 0 тоже надо вставить
        totalResultCum.extend(resultCum)
    
  res1_0 = [1 if x>threshold else 0 for x in totalResultCum]    

  return res1_0

res = check()

print(len(res), len(dfTst))

#from tensorflow.keras.models import save_model
#from keras.models import load_model

#model1.save('my_model1.h5')  
#model2.save('my_model2.h5')

#model = load_model('my_model.h5')

dfTst['y']=res
dfTst.to_csv('submission874.csv')

# ф-ция для определения индекса, в котором будет 1

def resCaseCalc(x):
  # свыше значения в 1.5 - записываем в "правильные ответы"
  if x > threshold:
    return 1.0
  else:
    return 0.0

# вариация с 2мя моделями
def countF1(threshold = 5.02, window1 = 10, window2 = 8, scaler1 = scaler10, scaler2 = scaler8):

  totalScore = []
  totalTrueVals = []
  totalResultCum = []
  for i in range(237,275): #275
    tempX = []
    tempIdxs = []
    testLstX1 = []
    testLstIdx1 = []
    testLstX2 = []
    testLstIdx2 = []
    tempX.extend(dfBase[55084:][dfBase['id']==i]['x'].values) # формируем данные для подачи в нс
    tempIdxs.extend(dfBase[55084:][dfBase['id']==i].index.values)

    for j in range(0,len(tempX)-window1+1, 1):
      
      t = scaler1.transform([tempX[j:j + window1]]) 
      testLstX1.append(t)
      testLstIdx1.append(tempIdxs[j:j+window1])

    trueVals = dfBase[dfBase['id']==i]['y'].values # все реальные значения в этом кейсе
    trueValsIndx = dfBase[dfBase['id']==i]['y'].index.values

    resultCum = [0.0 for el in range(len(trueVals))]
    for k in range(len(testLstX1)): # для каждого из окон (8 значений со смещением 1)
      if len(testLstX1[k])!=0:
        testLstX1[k] = testLstX1[k].reshape((testLstX1[k].shape[0],testLstX1[k].shape[1],1))
        result1 = model1.predict(testLstX1[k])[0][0] # получаем предикты по окнам из 8 значений для одного кейса за раз.
        
        resultWindowSpread1 = np.concatenate([[0.0 for i in range(k)],[result1 for i in range(window1)],[0.0 for i in range(k+window1,len(trueVals))]])
        
        resultCum = resultCum + resultWindowSpread1 

    for j in range(0,len(tempX)-window2+1, 1):
      
      t = scaler2.transform([tempX[j:j + window2]]) 
      testLstX2.append(t)
      testLstIdx2.append(tempIdxs[j:j+window2])

    for k in range(len(testLstX2)): # для каждого из окон (8 значений со смещением 1)
      if len(testLstX2[k])!=0:
        testLstX2[k] = testLstX2[k].reshape((testLstX2[k].shape[0],testLstX2[k].shape[1],1))
        
        result2 = model2.predict(testLstX2[k])[0][0] # 2я модель с другим окном
       
        resultWindowSpread2 = np.concatenate([[0.0 for i in range(k)],[result2 for i in range(window2)],[0.0 for i in range(k+window2,len(trueVals))]])
        resultCum = resultCum + resultWindowSpread2

    totalTrueVals.append(trueVals)
    totalResultCum.append(resultCum)
    #resCase = list(map(resCaseCalc, resultCum))
    
    #score = f1_score(trueVals, resCase, average='binary')
     
    #totalScore.append(score)

  return totalTrueVals, totalResultCum#, totalScore

# Сперва запускаешь это

threshold = 6
with warnings.catch_warnings():
    warnings.simplefilter("ignore")
    totalTrueVals, totalResultCum = countF1(threshold = threshold, window1 = 11, window2 = 7, scaler1 = scaler11, scaler2 = scaler7)

# посчитаем точность общую

tr=[]
pr=[]

for i in range(len(totalTrueVals)):
  tr.extend(totalTrueVals[i])
  pr.extend(totalResultCum[i])

thresholdVals = [i/100 for i in range(100, 750, 1)]
for i in range(len(thresholdVals)):

  pr1 = [1 if x>thresholdVals[i] else 0 for x in pr]

  print(f1_score(tr, pr1), thresholdVals[i])

# window = 9, 3rd model

for i in range(10):
  fig = plt.figure(figsize=(10, 5))

  plt.plot(totalTrueVals[i])  
  plt.plot(totalResultCum[i])

# window = 8, 2nd model

for i in range(10):
  fig = plt.figure(figsize=(10, 5))

  plt.plot(totalTrueVals[i])  
  plt.plot(totalResultCum[i])

# window 10 + window 8 (model1 and model2) - for presentation

for i in range(10):
  fig = plt.figure(figsize=(10, 5))

  plt.plot(totalTrueVals[i])  
  plt.plot(totalResultCum[i])

# window 8 + window 6, (model1 and model2)

for i in range(10):
  fig = plt.figure(figsize=(10, 5))

  plt.plot(totalTrueVals[i])  
  plt.plot(totalResultCum[i])

# window 8 and window 10

for i in range(10):
  fig = plt.figure(figsize=(10, 5))

  plt.plot(totalTrueVals[i])  
  plt.plot(totalResultCum[i])

import pandas as pd

d = pd.read_csv('/content/submission899.csv')

temp = []
for z in range(len(pr1)):
  
  if pr1[z]==1:
    temp.append(z)
  elif pr1[z]==0:
    dl = len(temp)
    temp = []
    if dl > 0 and dl < 4:
      pr1[z-dl:z]=[0 for el in range(dl)]

pr1=d['y'].values

d.drop(labels = 'y', axis = 1, inplace = True)

d['y'] = pr1

d.to_csv('submission90.csv')